{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "import dspy\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from src.html import clean_html\n",
    "from src.utils import html_str2md\n",
    "from src.models import Recipe\n",
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "LITELLM_API_KEY = os.getenv(\"LITELLM_API_KEY\")\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "LITELLM_URL = os.getenv(\"LITELLM_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dspy setup\n",
    "\n",
    "lm = dspy.LM(\n",
    "    MODEL,\n",
    "    api_base=LITELLM_URL,\n",
    "    api_key=LITELLM_API_KEY,\n",
    "    temperature=0,\n",
    "    max_tokens=8192,\n",
    ")\n",
    "dspy.settings.configure(lm=lm, async_max_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_file(file_path: Path) -> dict:\n",
    "    try:\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            soup = BeautifulSoup(f, \"html.parser\")\n",
    "        return {\n",
    "            \"file_path\": file_path,\n",
    "            \"method\": file_path.parent.name,\n",
    "            \"file_name\": file_path.name,\n",
    "            \"raw_html\": soup.prettify(),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_html_dataset(dataset_path: str) -> pd.DataFrame:\n",
    "    dataset_path = Path(dataset_path)\n",
    "    html_files = list(dataset_path.rglob(\"*.html\"))\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_html_file, html_files))\n",
    "    valid_results = [r for r in results if r is not None]\n",
    "    df = pd.DataFrame(valid_results)\n",
    "    df[\"cleaned_html\"] = df[\"raw_html\"].apply(clean_html)\n",
    "    df[\"markdown\"] = df[\"cleaned_html\"].apply(html_str2md)\n",
    "    return df\n",
    "\n",
    "\n",
    "dataset_path = \"src/data/generated/dummy\"\n",
    "df = load_html_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeSearchSignature(dspy.Signature):\n",
    "    \"\"\"Extract all recipes with ingredients and instructions from a text passage\"\"\"\n",
    "\n",
    "    passage: str = dspy.InputField(desc=\"a text passage\")\n",
    "    recipe: Recipe = dspy.OutputField(desc=\"Response with extracted recipe\")\n",
    "\n",
    "\n",
    "class RecipeExtractor(dspy.Module):\n",
    "    def __init__(self) -> None:\n",
    "        self.extract = dspy.Predict(RecipeSearchSignature)\n",
    "\n",
    "    def forward(\n",
    "        self, passage: str, file_path: str, method: str, file_name: str, input_type: str\n",
    "    ) -> str:\n",
    "        response = self.extract(passage=passage)\n",
    "        return dspy.Example(\n",
    "            file_path=file_path,\n",
    "            method=method,\n",
    "            file_name=file_name,\n",
    "            input_type=input_type,\n",
    "            response=response.recipe,\n",
    "        )\n",
    "\n",
    "\n",
    "batch = []\n",
    "for _, row in df.iterrows():\n",
    "    example = dspy.Example(\n",
    "        passage=row[\"markdown\"],\n",
    "        file_path=row[\"file_path\"],\n",
    "        method=row[\"method\"],\n",
    "        input_type=\"markdown\",\n",
    "        file_name=row[\"file_name\"],\n",
    "    ).with_inputs(\"passage\", \"file_path\", \"method\", \"file_name\", \"input_type\")\n",
    "    batch.append(example)\n",
    "\n",
    "    example = dspy.Example(\n",
    "        passage=row[\"cleaned_html\"],\n",
    "        file_path=row[\"file_path\"],\n",
    "        method=row[\"method\"],\n",
    "        input_type=\"html\",\n",
    "        file_name=row[\"file_name\"],\n",
    "    ).with_inputs(\"passage\", \"file_path\", \"method\", \"file_name\", \"input_type\")\n",
    "    batch.append(example)\n",
    "\n",
    "extractor = RecipeExtractor()\n",
    "metric = lambda x, y: True  # noqa: E731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = dspy.Evaluate(\n",
    "    devset=batch,\n",
    "    metric=metric,\n",
    "    num_threads=20,\n",
    "    display_progress=True,\n",
    "    return_outputs=True,\n",
    ")(extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for i in range(len(output[1])):\n",
    "    d = dict(output[1][i][1])\n",
    "    d[\"response\"] = d[\"response\"].model_dump()\n",
    "    rows.append(d)\n",
    "r = pd.DataFrame(rows)\n",
    "\n",
    "true_responses = (\n",
    "    r[(r[\"method\"] == \"true\") & (r[\"input_type\"] == \"html\")]\n",
    "    .set_index(\"file_name\")[\"response\"]\n",
    "    .to_dict()\n",
    ")\n",
    "r[\"response_true\"] = r[\"file_name\"].map(true_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"description\": \"Press tofu for 15 minutes to remove excess water\"}, {\"description\": \"Cut into 1-inch cubes\"}, {\"description\": \"Toss with cornstarch, salt, and pepper\"}, {\"description\": \"Heat sesame oil in a large skillet over medium-high heat\"}, {\"description\": \"Cook tofu 3-4 minutes per side until golden\"}, {\"description\": \"Remove and set aside\"}, {\"description\": \"Whisk all sauce ingredients in a bowl\"}, {\"description\": \"Set aside until needed\"}, {\"description\": \"Add oil to the same pan\"}, {\"description\": \"Stir-fry garlic and ginger until fragrant\"}, {\"description\": \"Add vegetables in order of cooking time\"}, {\"description\": \"Cook until crisp-tender\"}, {\"description\": \"Return tofu to pan\"}, {\"description\": \"Pour sauce over\"}, {\"description\": \"Simmer until thickened\"}, {\"description\": \"Sprinkle with sesame seeds\"}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json.dumps(true_responses)\n",
    "\n",
    "arr = [\n",
    "\"Press tofu for 15 minutes to remove excess water\",\n",
    "\"Cut into 1-inch cubes\",\n",
    "\"Toss with cornstarch, salt, and pepper\",\n",
    "\"Heat sesame oil in a large skillet over medium-high heat\",\n",
    "\"Cook tofu 3-4 minutes per side until golden\",\n",
    "\"Remove and set aside\",\n",
    "\"Whisk all sauce ingredients in a bowl\",\n",
    "\"Set aside until needed\",\n",
    "\"Add oil to the same pan\",\n",
    "\"Stir-fry garlic and ginger until fragrant\",\n",
    "\"Add vegetables in order of cooking time\",\n",
    "\"Cook until crisp-tender\",\n",
    "\"Return tofu to pan\",\n",
    "\"Pour sauce over\",\n",
    "\"Simmer until thickened\",\n",
    "\"Sprinkle with sesame seeds\",\n",
    "]\n",
    "l = []\n",
    "for a in arr:\n",
    "    l.append({\"description\": a} )\n",
    "print(json.dumps(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_recipes(true: dict, pred: dict) -> list[str, float]:\n",
    "    def format_ingredient(ingredient: dict) -> str:\n",
    "        return f\"{str(ingredient['amount'])} {ingredient['unit']} {ingredient['item']}\".replace(\n",
    "            \"None\", \"\"\n",
    "        ).strip()\n",
    "\n",
    "    def compare_lists(true_list: list[str], pred_list: list[str]) -> tuple[bool, float]:\n",
    "        from statistics import mean\n",
    "\n",
    "        is_match = len(true_list) == len(pred_list) and all(\n",
    "            t == p for t, p in zip(true_list, pred_list)\n",
    "        )\n",
    "        if pred_list == []:\n",
    "            avg_distance = 0\n",
    "        else:\n",
    "            avg_distance = (\n",
    "                mean(distance(t, p) for t, p in zip(true_list, pred_list))\n",
    "                if true_list\n",
    "                else 0\n",
    "            )\n",
    "        return is_match, avg_distance\n",
    "\n",
    "    # Compare titles\n",
    "    scores = {\n",
    "        \"title_match\": true[\"title\"] == pred[\"title\"],\n",
    "        \"title_distance\": distance(true[\"title\"], pred[\"title\"]),\n",
    "    }\n",
    "\n",
    "    # Compare ingredients\n",
    "    true_ingredients = [format_ingredient(i) for i in true[\"ingredients\"]]\n",
    "    pred_ingredients = [format_ingredient(i) for i in pred[\"ingredients\"]]\n",
    "\n",
    "    ingredients_match, ingredients_distance = compare_lists(\n",
    "        true_ingredients, pred_ingredients\n",
    "    )\n",
    "    scores.update(\n",
    "        {\n",
    "            \"ingredients_match\": ingredients_match,\n",
    "            \"ingredients_distance\": ingredients_distance,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Compare instructions\n",
    "    true_instructions = [i[\"description\"] for i in true[\"instructions\"]]\n",
    "    pred_instructions = [i[\"description\"] for i in pred[\"instructions\"]]\n",
    "    instructions_match, instructions_distance = compare_lists(\n",
    "        true_instructions, pred_instructions\n",
    "    )\n",
    "    scores.update(\n",
    "        {\n",
    "            \"instructions_match\": instructions_match,\n",
    "            \"instructions_distance\": instructions_distance,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "r[\"scores\"] = r.apply(lambda x: eval_recipes(x[\"response_true\"], x[\"response\"]), axis=1)\n",
    "r = pd.concat([r.drop([\"scores\"], axis=1), pd.json_normalize(r[\"scores\"])], axis=1)\n",
    "\n",
    "r[\n",
    "    [\n",
    "        \"input_type\",\n",
    "        \"method\",\n",
    "        \"title_match\",\n",
    "        \"title_distance\",\n",
    "        \"ingredients_match\",\n",
    "        \"ingredients_distance\",\n",
    "        \"instructions_match\",\n",
    "        \"instructions_distance\",\n",
    "    ]\n",
    "].groupby([\"method\", \"input_type\"]).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
